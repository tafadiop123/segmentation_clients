# -*- coding: utf-8 -*-
"""AI in Marketing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0eabg5NlCBBtKOtqyPjhDWyPt6QBkuf

# TASK #1: Comprendre l'état du problème et le Business Case

![alt text](https://drive.google.com/uc?id=1XUcy70cUux6E0VWzUsNsoaMkJxrwyFQv)

![alt text](https://drive.google.com/uc?id=1QX1dcuoz0LIFcIMSMkl61NpL1q_C8IaH)

---> Le marketing est essentiel pour la croissance et la faisabilité du commerce en détail

---> Les marketeurs peuveunt aider à construire la marque d'une compagnie, d'acquérir des clients, d'accroître les revenues et les ventes.

* EDUCATION : Les marketeurs éduquent et communiquent la proposition de valeur d'un produit à ses clients.

* ENGAGEMENT : Les marketeurs acquièrent de nouveaux clients et comprennent leurs besoins

* VENTE : Les marketeurs stimulent les ventes et le traffic vers les produits ou services.

* CROISSANCE : Les marketeurs assurent la croissance du business en atteignant de nouveaux clients.

![alt text](https://drive.google.com/uc?id=1p8QU7VCmLO0LokCDNmDjwxKppQUgQEgJ)

* L'un des points principaux de la douleur du marketeur est de comprendre ses clients et leurs besoins.

* En comprenant le client, les marketeurs peuvent lancer des campagnes de marketing ciblées qui sont taillées pour des besoins spécifiques.

* Si les données concernant les clients sont disponibles, alors la data science et l'IA/ML peuvent être utilisés pour effectuer la segmentation du marché.

* Ici on devra créer une campagne marketing en divisant les clients en 3 groupes disctints

![alt text](https://drive.google.com/uc?id=13siOTV0Vdxy0GzX1wSabOvBepsp53EAL)

## LES VARIABLES 

* ORDERNUMBER : Identification de la Commande Passée 
* QUANTITYORDERED : Le nombre d'articles commandés
* PRICEEACH : Le prix de chaque article
* SALES : Le montant total des ventes 
* ORDERDATE : Date à laquelle la commande est passée
* STATUS : Le status de la commande
* QTR_ID : Le trimestre dans lequel la commande est passée 
* MONTH_ID : Le mois dans lequel la commande est passée
* YEAR_ID : L'année dans laquelle la commande est passée
* PRODUCTLINE : La catégorie du produit
* CUSTOMERNAME : Le nom du client
* PHONENUMBER : Le numéro de téléphone
* ADDRESSLINE1 : L'addresse de livraison
* ADDRESSLINE2 : L'addresse de livraison
* CITY : La ville dans laquelle le client réside
* STATE : L'état dans lequel le client réside
* POSTALCODE : Le code postal dans lequel le client réside
* COUNTRY : Le pays dans lequel le client réside
* TERRITORY : le territoire dans lequel le client réside
* DEALSIZE : La taille de la commande
* CONTACTFIRST_NAME : Le prénom de la personne de contact
* CONTACTLAST_NAME : Le nom de la personne de contact

![alt text](https://drive.google.com/uc?id=1-WzLecExInr9WH_nPh51pT2kkojuZ_Ob)

Data Source : https://www.kaggle.com/kyanyoga/sample-sales-data

# TASK #2: IMPORT LIBRARIES AND DATASETS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import zipfile
import cv2
import plotly.express as px
import tensorflow as tf
from tensorflow.python.keras import Sequential
from tensorflow.keras import layers, optimizers
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
from IPython.display import display
from tensorflow.keras import backend as K
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
import plotly.express as px
import plotly.graph_objects as go

from google.colab import files #library to upload files to colab notebook
# %matplotlib inline

# Mount the drive
from google.colab import drive
drive.mount('/content/drive')

sales_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Modern Artificial Intelligence Masterclass UDEMY/AI_Business_Marketing/AI in Marketing Dataset/sales_data_sample.csv', encoding = 'unicode_escape')
# Note: MSRP est le prix de détail suggéré par le fabricant (Manufacturer's Suggested Retail Price) ou le prix de l'autocollant représente le prix de détail suggéré des produits. 
# MSRP est utilisé pour uniformiser le prix des produits sur plusieurs sites de magasins de l'entreprise.

sales_df

# Let's view the types of data
sales_df.dtypes

"""MINI CHALLENGE #1: 
- Convert order date into date time format using Pandas
- Verify that the conversion was successful by printing out the datatype
"""

# Convert order date to datetime format
sales_df['ORDERDATE'] = pd.to_datetime(sales_df['ORDERDATE'])
# Check the type of data
sales_df.dtypes

"""MINI CHALLENGE #2: 
- How many null elements exist in 'ADDRESSLINE2'? 
"""

# Check the number of non-null values in the dataframe
sales_df.info()

# Check the number of Null values in the data
sales_df.isnull().sum()

# il y'a trop de valeurs nulles dans les variables 'addressline2', 'state', 'postal code' and 'territory' on peut les supprimer. 
# La variable "Country" représenterait l'information géographique de la commande .
# Ainsi on peut supprimer les variables "city, address1, phone number, contact_name, contact last_name and contact first_name" puisqu'ils ne sont pas nécessaires à l'analyse.

df_drop  = ['ADDRESSLINE1', 'ADDRESSLINE2', 'POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']
sales_df = sales_df.drop(df_drop, axis = 1)
sales_df.head()

sales_df.isnull().sum()

"""MINI CHALLENGE #3: 
- How many unique values exist in 'country'?
- How many unique product codes and product lines do we have?
"""

# Obtain the number of unique values in each column
sales_df.nunique()

"""# TASK #3: PERFORM EXPLORATORY DATA ANALYSIS AND DATA CLEANING - PART #1"""

sales_df['COUNTRY'].value_counts().index

sales_df['COUNTRY'].value_counts()

# On crée une fonction pour visualiser le comptage des articles dans la colonne donnée.
# On note que Plotly est une bibliothèque graphique Python qui permet de réaliser des graphiques interactifs de qualité. 
# Voir: https://plotly.com/python/
# Note: px (plotly_express) est une enveloppe de haut niveau autour de Plotly pour l'exploration rapide des données et la génération de figures
# Voir: https://plotly.github.io/plotly_express

def barplot_visualization(x):
  fig = plt.Figure(figsize = (12, 6))
  fig = px.bar(x = sales_df[x].value_counts().index, y = sales_df[x].value_counts(), color = sales_df[x].value_counts().index, height = 600)
  fig.show()

# Let's call this function for any given column such as 'COUNTRY'
barplot_visualization('COUNTRY')

"""MINI CHALLENGE #4: 
- How many unique order status values do we have? 
- Do we have a balanced datasets? comment on the result and propose solutions to fix the issue
"""

barplot_visualization('STATUS')

sales_df.drop(columns= ['STATUS'], inplace = True)
sales_df

barplot_visualization('PRODUCTLINE')

barplot_visualization('DEALSIZE')

# Function to add dummy variables to replace categorical variables

def dummies(x):
  dummy = pd.get_dummies(sales_df[x])
  sales_df.drop(columns = x , inplace = True)
  return pd.concat([sales_df, dummy], axis = 1)

# Let's obtain dummy variables for the column 'COUNTRY'
sales_df = dummies('COUNTRY')
sales_df

"""MINI CHALLENGE #5: 
- Obtain dummies for the product line and deal size columns
- Perform a sanity check and see if the transformation was successfull
"""

sales_df = dummies('PRODUCTLINE')

sales_df

sales_df = dummies('DEALSIZE')
sales_df

sales_df

y = pd.Categorical(sales_df['PRODUCTCODE'])
y

y = pd.Categorical(sales_df['PRODUCTCODE']).codes
y

# Comme le code produit unique du numéro est 109, si l'on ajoute les variables "one-hot",
# il y'aurait 109 colonnes supplémentaires, nous pouvons éviter cela en utilisant un encodage par catégorie.
# Ce n'est pas la manière optimale de la traiter, mais il est important d'éviter la malédiction de la dimensionnalité
sales_df['PRODUCTCODE'] = pd.Categorical(sales_df['PRODUCTCODE']).codes

sales_df

sales_df

"""# TASK #4: PERFORM EXPLORATORY DATA ANALYSIS AND DATA CLEANING - PART #2"""

# Group data by order date
sales_df_group = sales_df.groupby(by = "ORDERDATE").sum()
sales_df_group

"""MINI CHALLENGE #6: 
- Based on the data, When does the sales generally peak (which month)?  
- Support your answer with visualizations/plots
"""

fig = px.line(x = sales_df_group.index, y = sales_df_group.SALES, title = 'Sales')
fig.show()

# We can drop 'ORDERDATE' and keep the rest of the date-related data such as 'MONTH'
sales_df.drop("ORDERDATE", axis = 1, inplace = True)
sales_df.shape

"""MINI CHALLENGE #7: 
- Plot the correlation matrix between variables
- Comment on the matrix results. 
"""

plt.figure(figsize = (20, 20))
corr_matrix = sales_df.iloc[:, :10].corr()
sns.heatmap(corr_matrix, annot = True, cbar = False)

# It looks like the Quarter ID and the monthly IDs are highly correlated
# Let's drop 'QTR_ID' (or 'MONTH_ID') 
sales_df.drop("QTR_ID", axis = 1, inplace = True)
sales_df.shape

# On représente les "distplots"
# Distplot montre (1) histogram, (2) kde plot and (3) rug plot.
# (1) Histogram: il s'agit d'une représentation graphique des données à l'aide de barres de différentes hauteurs. Chaque barre regroupe des nombres en fourchettes et les barres plus hautes indiquent que davantage de données se trouvent dans cette fourchette.
# (2) Kde Plot: Le "Kernel Density Estimate" est utilisé pour visualiser la densité de probabilité d'une variable continue.
# (3) Rug plot: un diagramme de données pour une variable quantitative unique, affiché sous forme de marques le long d'un axe (diagramme de dispersion unidimensionnel). 

import plotly.figure_factory as ff

plt.figure(figsize = (10, 10))

for i in range(8):
  if sales_df.columns[i] != 'ORDERLINENUMBER':
    fig = ff.create_distplot([sales_df[sales_df.columns[i]].apply(lambda x: float(x))], ['distplot'])
    fig.update_layout(title_text = sales_df.columns[i])
    fig.show()

# Visualize the relationship between variables using pairplots
plt.figure(figsize = (15, 15))

fig = px.scatter_matrix(sales_df,
    dimensions = sales_df.columns[:8], color = 'MONTH_ID')

fig.update_layout(
    title = 'Sales Data',
    width = 1100,
    height = 1100,
)
fig.show()

# Une tendance existe entre les variables 'SALES' et 'QUANTITYORDERED'  
# Une tendance existe entre les variables 'MSRP' and 'PRICEEACH'  
# Une tendance existe entre les variables 'PRICEEACH' and 'SALES'
# Il semble que la croissance des ventes existe au fur et à mesure que nous passons de 2013 à 2014 à 2015 ('SALES' vs. 'YEAR_ID')
# zoomer sur les variables 'SALES' et 'QUANTITYORDERED', vous pourrez voir les informations mensuelles codées par couleur sur le graphique

"""# TASK #5: UNDERSTAND THE THEORY AND INTUITION BEHIND K-MEANS CLUSTERING

## K-Means Définition 

* Le K-means est un algorithme d'apprentissage non-supervisé (Clustering)
* Le K-means marche en groupant quelques points de données ensemble (clustering) de manière non-supervisé. 
* L'algorithme regroupe les observations avec des valeurs similaires en mesurant la distane euclidienne entre les points.

![alt text](https://drive.google.com/uc?id=12bNt86wOzi8h5kvO30ui-e-36D9oTvQp)

## Les étapes de l'algorithme K-means
1. Choisir le nombre de clusters "K"
2. Sélectionner des points aléatoires "K" qui seront les centroïdes de chaque cluster
3. Assigner point de donnée à un centroïd le plus proche, ainsi fait cela va permettre de créer un nombre "K" de clusters
4. Calculer un nouveau centroïde pour chaque cluster
5. Reassigner chaque point de donnée au nouveau centroïde créé
6. Partir à l'étape 4 et recommencer

![alt text](https://drive.google.com/uc?id=1Ate7n0b1lY_zNRfubijr7QeOfz-MdufY)

![alt text](https://drive.google.com/uc?id=1Mfd-dqo-toqDwXqEEoIh6QCX2CwPTbLx)

![alt text](https://drive.google.com/uc?id=13nudLLz_G7uQbqvgJlqV3sVmvt1JBoJs)

![alt text](https://drive.google.com/uc?id=1ord3m3hqIZcjPjiHf2zkb_YFL5SqAPp8)

![alt text](https://drive.google.com/uc?id=1M6MGEPTG22ZOXgGbKmuujCmNq3zT3HWO)

![alt text](https://drive.google.com/uc?id=16YDfY_-rsVNEZ2S8mHct-UPPkMwtWd6J)

![alt text](https://drive.google.com/uc?id=1VGfHIIBheXG_p4YmvA1yaGTnXJaW8LsZ)

# TASK #6: FIND THE OPTIMAL NUMBER OF CLUSTERS USING ELBOW METHOD

- La méthode du coude (Elbow Method) est une méthode heuristique d'interprétation et de validation de la cohérence au sein de l'analyse des clusters conçue pour aider à trouver le nombre approprié de clusters dans un ensemble de données.
- Si le graphique linéaire ressemble à un bras, alors le "coude" sur le bras est la valeur de k qui est la meilleure.
- Source: 
  - https://en.wikipedia.org/wiki/Elbow_method_(clustering)
  - https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/
"""

# Scale the data
# Ici on normalise les données en supprimant la moyenne et en divisant par la variance
scaler = StandardScaler()
sales_df_scaled = scaler.fit_transform(sales_df)

sales_df_scaled.shape

scores = []

range_values = range(1, 15)

for i in range_values:
  kmeans = KMeans(n_clusters = i)
  kmeans.fit(sales_df_scaled)
  scores.append(kmeans.inertia_) # "intertia_" est la somme des carrés des distances des échantillons à leur centre de grappe le plus proche

plt.plot(scores, 'bx-')
plt.title('Finding right number of clusters')
plt.xlabel('Clusters')
plt.ylabel('scores') 
plt.show()

# Kmeans details in Sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
# Ici la courbe est linéaire elle n'est pas en forme de bras mais on va prendre K=5 pour voire.
# Note that curve will change everytime we run the cell

"""# TASK #7: APPLY K-MEANS METHOD"""

# Cluster the data using k-means
kmeans = KMeans(5)
kmeans.fit(sales_df_scaled)
labels = kmeans.labels_

labels

kmeans.cluster_centers_.shape

# Let's take a look at the cluster centers 
cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [sales_df.columns])
cluster_centers

# In order to understand what these numbers mean, let's perform inverse transformation
cluster_centers = scaler.inverse_transform(cluster_centers)
cluster_centers = pd.DataFrame(data = cluster_centers, columns = [sales_df.columns])
cluster_centers

# Cluster 0 - Ce groupe représente les clients qui achètent des articles en quantité variable ~34, ils ont tendance à acheter des articles à prix moyen ~83. Leurs ventes sont un peu meilleures en moyenne ~3556.
# Cluster 1 (Highest)- Ce groupe représente les clients qui achètent des articles en grande quantité centrés autour de ~45, ils achètent des articles dans toutes les gammes de prix, en penchant vers les articles à prix élevé de ~99. Ils correspondent également aux ventes totales les plus élevées vers ~7687 et ils sont actifs tout au long de l'année. Ils sont les plus gros acheteurs de produits ayant un MSRP élevé ~153
# Cluster 2 - Ce groupe représente les clients qui achètent des articles en quantité variable ~38, ils ont tendance à acheter des articles au prix moyen ~95. Leurs ventes ~4404 et ils achètent des produits dont le MSRP est le deuxième plus élevé ~115.
# Cluster 3 (Lowest)- Ce groupe représente les clients qui achètent des articles en faible quantité ~30. Ils ont tendance à acheter des articles à bas prix ~68. Leur chiffre d'affaires ~2059 est inférieur à celui des autres groupes et ils sont extrêmement actifs pendant la période des vacances. Ils achètent des produits à faible MSRP ~77.
# Cluster 4 - Ce groupe représente les clients qui achètent des articles en quantité variable ~34, ils ont tendance à acheter des articles à prix moyen ~82. Leurs ventes sont un peu meilleures en moyenne ~3169.

labels.shape # Labels associated to each data pointa

labels.max()

labels.min()

y_kmeans = kmeans.fit_predict(sales_df_scaled)
y_kmeans

y_kmeans.shape

# Add a label (which cluster) corresponding to each data point
sale_df_cluster = pd.concat([sales_df, pd.DataFrame({'cluster':labels})], axis = 1)
sale_df_cluster

sales_df['ORDERLINENUMBER'] = sales_df['ORDERLINENUMBER'].apply(lambda x: float(x))

# plot histogram for each feature based on cluster 
for i in sales_df.columns[:8]:
  plt.figure(figsize = (30, 6))
  for j in range(5):
    plt.subplot(1, 5, j+1)
    cluster = sale_df_cluster[sale_df_cluster['cluster'] == j]
    cluster[i].hist()
    plt.title('{}    \nCluster - {} '.format(i,j))
  
  plt.show()

"""# TASK #8: APPLY PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS

## Analyse en Composant Prinicipal (PCA) : Vue d'ensemble
* Le PCA est algorithme de machine learning non-supervisé 
* Le PCA effectue la réduction de dimensions en essayant de garder les informations inchangées
* Le PCA marche en essayant de trouver un nouvel ensemble de variables appelé "Composant"
* Les composants sont composés des variables d'entrée données non-corrélées.

![alt text](https://drive.google.com/uc?id=1DuqbJRlxzbm37fPD3ZKxLGzQ2PbstZt8)
"""

# Reduce the original data to 3 dimensions using PCA for visualizig the clusters
pca = PCA(n_components = 3)
principal_comp = pca.fit_transform(sales_df_scaled)
principal_comp

pca_df = pd.DataFrame(data = principal_comp, columns = ['pca1', 'pca2', 'pca3'])
pca_df.head()

# Concatenate the clusters labels to the dataframe
pca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis = 1)
pca_df

# Visualize clusters using 3D-Scatterplot
fig = px.scatter_3d(pca_df, x = 'pca1', y = 'pca2', z = 'pca3', color = 'cluster', symbol = 'cluster', size_max = 18, opacity = 0.7)
fig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))

"""MINI CHALLENGE #8:
- Change the number of components to 2, modify the code and rerun the model
- Visualize the clusters using 2D plots
"""

# Reduce the original data to 3 dimensions using PCA for visualizig the clusters
pca2 = PCA(n_components = 2)
principal_comp2 = pca2.fit_transform(sales_df_scaled)
principal_comp2

pca_df2 = pd.DataFrame(data = principal_comp2, columns = ['pca1', 'pca2'])
pca_df2.head()

# Concatenate the clusters labels to the dataframe
pca_df2 = pd.concat([pca_df2, pd.DataFrame({'cluster':labels})], axis = 1)
pca_df2

# Représentation sur 2 Dimensions
ax = sns.scatterplot(x = "pca1", y = "pca2", hue = "cluster", data = pca_df2, palette = ["red", "green", "blue", "pink", "yellow"])
plt.show()

"""# TASK #9: UNDERSTAND THE THEORY AND INTUITION BEHIND AUTOENCODERS

![alt text](https://drive.google.com/uc?id=1IgLu2F0X2pFfsWtjNsgJVozXm-bMAJrd)

![alt text](https://drive.google.com/uc?id=1mqB2MppbYc0-2j-oWkQdAfpVfiLkLweM)

![alt text](https://drive.google.com/uc?id=1YNb54bws2fVqSn8KLdo5NujBaGz9Bmvw)

# TASK #10: APPLY AUTOENCODERS (PERFORM DIMENSIONALITY REDUCTION USING AUTOENCODERS)
"""

sales_df.shape

sales_df.head()

# from keras.optimizers import SGD

# Glorot Uniform initializer: https://keras.rstudio.com/reference/initializer_glorot_uniform.html

input_df = Input(shape = (37,))
x = Dense(50, activation = 'relu')(input_df)
x = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)
x = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)
x = Dense(2000, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)
encoded = Dense(8, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)
x = Dense(2000, activation = 'relu', kernel_initializer = 'glorot_uniform')(encoded)
x = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)
decoded = Dense(37, kernel_initializer = 'glorot_uniform')(x)

# autoencoder
autoencoder = Model(input_df, decoded)

# encoder - used for dimensionality reduction
encoder = Model(input_df, encoded)

autoencoder.compile(optimizer = 'adam', loss='mean_squared_error')

autoencoder.fit(sales_df, sales_df, batch_size = 128, epochs = 500, verbose = 3)

autoencoder.save_weights('autoencoder_1.h5')

pred = encoder.predict(sales_df_scaled)

scores = []

range_values = range(1, 15)

for i in range_values:
  kmeans = KMeans(n_clusters = i)
  kmeans.fit(pred)
  scores.append(kmeans.inertia_)

plt.plot(scores, 'bx-')
plt.title('Finding right number of clusters')
plt.xlabel('Clusters')
plt.ylabel('scores') 
plt.show()

kmeans = KMeans(3)
kmeans.fit(pred)
labels = kmeans.labels_
y_kmeans = kmeans.fit_predict(sales_df_scaled)

df_cluster_dr = pd.concat([sales_df, pd.DataFrame({'cluster':labels})], axis = 1)
df_cluster_dr.head()

cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [sales_df.columns])
cluster_centers

cluster_centers = scaler.inverse_transform(cluster_centers)
cluster_centers = pd.DataFrame(data = cluster_centers, columns = [sales_df.columns])
cluster_centers

# plot histogram for each feature based on cluster 
for i in sales_df.columns[:8]:
  plt.figure(figsize = (30, 6))
  for j in range(3):
    plt.subplot(1, 3, j+1)
    cluster = df_cluster_dr[df_cluster_dr['cluster'] == j]
    cluster[i].hist()
    plt.title('{}    \nCluster - {} '.format(i,j))
  
  plt.show()

# Cluster 0 - This group represents customers who buy items in high quantity(47), they usually buy items with high prices(99). They bring-in more sales than other clusters. They are mostly active through out the year. They usually buy products corresponding to product code 10-90. They buy products with high mrsp(158).
# Cluster 1 - This group represents customers who buy items in average quantity(37) and they buy tend to buy high price items(95). They bring-in average sales(4398) and they are active all around the year.They are the highest buyers of products corresponding to product code 0-10 and 90-100.Also they prefer to buy products with high MSRP(115) .
# Cluster 2 - This group represents customers who buy items in small quantity(30), they tend to buy low price items(69). They correspond to the lowest total sale(2061) and they are active all around the year.They are the highest buyers of products corresponding to product code 0-20 and 100-110  they then to buy products with low MSRP(77).

# Reduce the original data to 3 dimension using PCA for visualize the clusters
pca = PCA(n_components = 3)
prin_comp = pca.fit_transform(sales_df_scaled)
pca_df = pd.DataFrame(data = prin_comp, columns = ['pca1', 'pca2', 'pca3'])
pca_df.head()

pca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis = 1)
pca_df.head()

# Visualize clusters using 3D-Scatterplot
fig = px.scatter_3d(pca_df, x = 'pca1', y = 'pca2', z = 'pca3',
              color='cluster', symbol = 'cluster', size_max = 10, opacity = 0.7)
fig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))

"""# EXCELLENT JOB! YOU SHOULD BE PROUD OF YOUR NEWLY ACQUIRED SKILLS

MINI CHALLENGE #1: 
- Convert order date into date time format using Pandas
- Verify that the conversion was successful by printing out the datatype
"""

# Convert order date to datetime format
sales_df['ORDERDATE'] = pd.to_datetime(sales_df['ORDERDATE'])
# Check the type of data
sales_df.dtypes

"""MINI CHALLENGE #2: 
- How many null elements exist in 'ADDRESSLINE2'? 
"""

# Check the number of non-null values in the dataframe
sales_df.info()

# Check the number of Null values in the data
sales_df.isnull().sum()

"""MINI CHALLENGE #3: 
- How many unique values exist in 'country'?
- How many unique product codes and product lines do we have?
"""

# Obtain the number of unique values in each column
sales_df.nunique()

"""MINI CHALLENGE #4: 
- How many unique order status values do we have? 
- Do we have a balanced datasets? comment on the result and propose solutions to fix the issue
"""

# Let's explore the 'STATUS' column
barplot_visualization('STATUS')

# The data is unbalanced and mostly contains 'shipped' status, let's remove the 'STATUS' column 
sales_df.drop(columns = ['STATUS'], inplace = True)
sales_df.shape